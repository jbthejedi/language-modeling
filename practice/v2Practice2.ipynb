{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "089b6ead-17e9-4d4c-8c9b-6c5114a52969",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x103c45eb0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "torch.manual_seed(1337)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "60a4cac6-e3d0-453b-821b-6a4ce5232527",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hii there\n"
     ]
    }
   ],
   "source": [
    "with Path('../input.txt').open('r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "vocab = sorted(list(set(text)))\n",
    "stoi = { ch: i for i, ch in enumerate(vocab) }\n",
    "itos = { i: ch for i, ch in enumerate(vocab) }\n",
    "encode = lambda x: [stoi[s] for s in x]\n",
    "decode = lambda x: [itos[s] for s in x]\n",
    "\n",
    "print(\"\".join(decode(encode(\"hii there\"))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6cc21fdc-56d6-4a2a-ac45-211e019d80ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "n = int(0.9 * len(data))\n",
    "train_data = data[n:]\n",
    "val_data = data[:n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4ccbc5c7-17f3-4218-856b-0346384216bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "block_size = 8\n",
    "n_embd = 12\n",
    "\n",
    "eval_iters = 200\n",
    "max_iters = 4000\n",
    "\n",
    "vocab_size = len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "752d8a92-72e8-435a-81d5-51540a64464c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.randint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c4580e39-667e-47e9-974e-c965e4c89c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(split, batch_size):\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2af3a3bf-908e-4355-bad7-c1a54e96e99d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 1., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 1., 1., 0., 0., 0., 0., 0.],\n",
       "        [1., 1., 1., 1., 0., 0., 0., 0.],\n",
       "        [1., 1., 1., 1., 1., 0., 0., 0.],\n",
       "        [1., 1., 1., 1., 1., 1., 0., 0.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 0.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B, T, C = 4, 8, 6\n",
    "x = torch.randn(B, T, C)\n",
    "tril = torch.tril(torch.ones(T, T))\n",
    "tril[:T, :T]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "496e066e-9086-4c55-9ed0-f49b35159206",
   "metadata": {},
   "outputs": [],
   "source": [
    "wei = torch.zeros(T, T)\n",
    "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
    "wei = F.softmax(wei, dim=-1)\n",
    "wei @ x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b18c7506-d673-4f60-9c4d-6911bafbd45f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    def __init__(self, head_size, block_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(head_size, head_size, bias=False)\n",
    "        self.query = nn.Linear(head_size, head_size, bias=False)\n",
    "        self.value = nn.Linear(head_size, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        k = self.key(x) # (B, T, C)\n",
    "        q = self.query(x) # (B, T, C)\n",
    "\n",
    "        wei = k @ q.transpose(-2, -1) * C**-0.5 # (B, T, T)\n",
    "\n",
    "        # Why we need `self.tril[:T, :T] == 0?\n",
    "        # Because in the forward pass, \\(T\\) (the current sequence length)\n",
    "        # can be less than or equal to `block_size`. The buffer `self.tril` is sized\n",
    "        # `(block_size, block_size)`, while `wei` is `(B, T, T)`.\n",
    "        # If you do `self.tril == 0`, you get a `(block_size, block_size)` mask, which\n",
    "        # will not match `(T, T)` for smaller \\(T\\).\n",
    "        # Using `self.tril[:T, :T] == 0` ensures the mask also has\n",
    "        # shape \\((T, T)\\) to match `wei`. This way, the model only attends up\n",
    "        # to the current sequence length \\(T\\), rather than the full `block_size`.\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
    "        wei = F.softmax(wei, dim=-1)\n",
    "\n",
    "        v = self.value(x) # (B, T, C)\n",
    "        out = wei @ v # (B, T, T)x(B, T, C) = (B, T, C)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a4b6d886-50c1-4d2a-a3ea-63fbaf3ffc98",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size, n_embd):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.sa_head = Head(head_size=n_embd, block_size=block_size)\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "        \n",
    "        tok_emb = self.token_embedding_table(idx) # (B, T, C)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device))\n",
    "        x = tok_emb + pos_emb # (B, T, C)\n",
    "        x = self.sa_head(x) # (B, T, C)\n",
    "        logits = self.lm_head(x) # (B, T, vocab_size)\n",
    "        \n",
    "        if targets is None:\n",
    "            losses = None\n",
    "        else:\n",
    "            # targets.shape when cross_entropy turns them\n",
    "            # to OHE --> (B, T, vocab_size).\n",
    "            # cross_entropy needs shape (minibatch, C).\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            losses = F.cross_entropy(logits, targets)\n",
    "            # F.softmax(logits, dim=-1)\n",
    "        return logits, losses\n",
    "\n",
    "    def generate(self, idx, max_new_tokens=1000):\n",
    "        for i in range(max_new_tokens):\n",
    "            # if i == 20:\n",
    "            #     break\n",
    "            # print(f\"idx.shape {\"idx.shape})\n",
    "            \n",
    "            # The context window is block_size,\n",
    "            # so we have to truncate the sequence to make sure\n",
    "            # we're not extendnig past block_size\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            # print(f\"idx_cond.shape {idx_cond.shape}\")\n",
    "            logits, _ = self(idx_cond)\n",
    "            # print(f\"before indexing {logits.shape}\")\n",
    "            logits = logits[:,-1,:]\n",
    "            # print(f\"after indexing {logits.shape}\")\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            idx = torch.cat([idx, idx_next], dim=1)\n",
    "            # print()\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b780e88c-1c4d-4858-82fc-6235ce81ca2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.2544, -0.1150, -0.1789,  ..., -0.2726,  0.7882, -0.3389],\n",
       "         [-0.2856, -0.1458, -0.0049,  ..., -0.0748,  0.4821,  0.1438],\n",
       "         [-0.1902, -0.1198,  0.0850,  ...,  0.0093,  0.3267,  0.4185],\n",
       "         ...,\n",
       "         [-0.1761,  0.1390,  0.1561,  ...,  0.1782,  0.1724,  0.1801],\n",
       "         [-0.4078, -0.0443,  0.0874,  ...,  0.0681,  0.1808,  0.2982],\n",
       "         [-0.6289, -0.0441,  0.0464,  ...,  0.0414,  0.1434,  0.2973]],\n",
       "        grad_fn=<ViewBackward0>),\n",
       " tensor(4.2712, grad_fn=<NllLossBackward0>))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = LanguageModel(vocab_size, n_embd)\n",
    "m = m.to(device)\n",
    "\n",
    "xb, yb = get_batch('train', batch_size)\n",
    "logits, losses = m(xb, yb)\n",
    "logits, losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5ef42302-2d0f-4a3e-8469-0e8d6a7f456a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "_in = torch.zeros((1, 1), dtype=torch.long)\n",
    "out = m.generate(_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f28d8467-a62b-4608-8bd8-7cfc4d700af3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ";GH?YDQXXHvC'efXg!'q!GQudrZs&&z?jRje;ubDXlyLQfFGKQXlzID-D3rEj&!ympinKW:XgZ,O,ZfIwkAumJsq\n",
      ".BTQlcMDE$LvErXtsp:?WE33Ni:Tlcl.LHjGvrs$rf;DT-q,nIERtN?;ujh'x!YHUiiNTNQ-dhtBI:G$zyPloIzZ&YQ.XyCOLLlvcPhyVqYzuejj?KCbDQJEjL.nL$d&3kbN3V\n",
      "WBR!ojpeqS'qD,KipfOgDa$PJWCZg;S'-z3hq-ngREqhtTNg3E3esYX;sqDMrPysuMyyOL$qm?&.vUwl znrR$HHhmlw.'CsQwBLm'sgTcGICQ'.BzFNF$Srx3HKEIsMgsslFGkWrVugRIdX\n",
      "hYpv!-jviiZTeRV MLCUliA!;UOYt!-ApoMyeYAmwDB?Ns!3sZyP$HIeWrcns:PBCOuF&&rib&Og NYOJV-ddsIuABSEXsW.L$&\n",
      "Urf3lAY;&W\n",
      "Ypkbg&zELg;Y yX'yt-F,su&lXza!Sx3nI?MW!IHi3kE.!EcxC,i3d.iEzz3RwKCpCZ&nOvufAQ BDEvPC$;m3!FN-PM?$Y3XMg,Ee-?A'kh,-us.xz'.UgUz&wCmBKtnGzZg&v;?TOv'P?qDCETzBd:$ w -LuwgiYXhh:E-wFnCJ;fL;gzDyLhrwXl,b,LoNh3W\n",
      "yvw3grfWa$EMVsYg,N&IU$UBdtLMKSNMuxB,P-33,?w-3QMlMWJya!,KY;m$f,INttRBZFsds$hFDcY;yg3BlbyunJEjznvHIRAQ.wYhxfgyxIUrHC.y UWv YJt,K\n",
      "Z\n",
      "J;3VLe-CT:VZm?! dTCyVVHeRSqDEXPpohX&v\n",
      "Q.UZ.\n",
      "$eh&qzRHIsjdisRr\n",
      "yjuxvvMzSW:n$\n",
      "3C&ym;ANWhElSmx.TDmo$MK\n",
      "SnfHyV:t&&We;DSX!VR-?r?yzvKGZZ!FPx'$SWjzsb:glDJsZDCwafMevqlmk&M!fKsC&hISnv?3IlI,SEklFqkfGbdnBZ\n"
     ]
    }
   ],
   "source": [
    "print(\"\".join(decode(out[0].tolist())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9854106f-1151-4e15-843d-2d269ce87169",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss(model, batch_size, eval_iters):\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in['train', 'val']:\n",
    "        losses_list = torch.zeros(eval_iters)\n",
    "        for i in range(eval_iters):\n",
    "            X, Y = get_batch(split, batch_size)\n",
    "            logits, losses = model(X, Y)\n",
    "            losses_list[i] = losses.item()\n",
    "        out[split] = losses_list.mean()\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b9ab73f9-258e-4157-89c0-9161f6b8e898",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss 4.207266807556152 val loss 4.216193675994873\n",
      "train loss 3.423068046569824 val loss 3.3658740520477295\n",
      "train loss 3.293736457824707 val loss 3.2521278858184814\n",
      "train loss 3.232297420501709 val loss 3.244847059249878\n",
      "train loss 3.128462314605713 val loss 3.1404178142547607\n",
      "train loss 3.0582430362701416 val loss 3.088736057281494\n",
      "train loss 2.9825069904327393 val loss 3.023178815841675\n",
      "train loss 2.9039318561553955 val loss 2.9639642238616943\n",
      "train loss 2.835329532623291 val loss 2.879398822784424\n",
      "train loss 2.765599250793457 val loss 2.863208293914795\n",
      "train loss 2.7834486961364746 val loss 2.801379442214966\n",
      "train loss 2.7111623287200928 val loss 2.7769906520843506\n",
      "train loss 2.7035605907440186 val loss 2.7881929874420166\n",
      "train loss 2.683370590209961 val loss 2.759216547012329\n",
      "train loss 2.6446056365966797 val loss 2.7420058250427246\n",
      "train loss 2.6271276473999023 val loss 2.727348566055298\n",
      "train loss 2.6083714962005615 val loss 2.6915433406829834\n",
      "train loss 2.6572649478912354 val loss 2.7054784297943115\n",
      "train loss 2.6106178760528564 val loss 2.7122714519500732\n",
      "train loss 2.6228160858154297 val loss 2.6972687244415283\n",
      "2.558610677719116\n"
     ]
    }
   ],
   "source": [
    "m = LanguageModel(vocab_size, n_embd)\n",
    "m = m.to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)\n",
    "\n",
    "for iter_ in range(max_iters):\n",
    "    if iter_ % eval_iters == 0:\n",
    "        out = estimate_loss(m, batch_size, eval_iters)\n",
    "        print(f\"train loss {out['train']} val loss {out['val']}\")\n",
    "    xb, yb = get_batch('train', batch_size)\n",
    "    logits, losses = m(xb, yb)\n",
    "    optimizer.zero_grad()\n",
    "    losses.backward()\n",
    "    optimizer.step()\n",
    "print(losses.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5de8d4b5-a7c7-4985-bbf4-940f7460209f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "AVo; tse?\n",
      "\n",
      "A!\n",
      "Toous ti?\n",
      "I-EI:\n",
      "OU:\n",
      "Avoce PUOyt h k; at apsn sishdta ty oey mf astl yroHhand mis raetanllles gehengyr shociteas gryaf I, ith fe yy it.\n",
      "B:\n",
      "Tcli?\n",
      "\n",
      "L\n",
      "Wyeean?\n",
      "\n",
      "\n",
      "Pyorg th belreon h sandt inshesins heense chart dllg su.\n",
      "\n",
      "Nathy theg?\n",
      "\n",
      "Seo RNIO:\n",
      "Wo sd se seml, pes hdsen o,\n",
      "\n",
      "THT::\n",
      "KP\n",
      "OSI loflres tanwou Kwof ud wgo thu ms:\n",
      "h sus becadesath dins\n",
      "Bag ce iosnthe th lile kat:\n",
      "Ehar;: m nthy.\n",
      "Nhe o athl he ts, withe b.\n",
      "\n",
      "\n",
      "\n",
      "O:e hyy y\n",
      "TI\n",
      "BSAAGL-O:\n",
      "EThotecoussh ild f o, t,\n",
      ":\n",
      "SA,\n",
      "ir t atese sry mlfee tho\n",
      "Hcy y sm, b dhoknon tEGl- sorreiroked tedl winrthinurshol tyesriche ina; pify, okrt sse alsint licore xyo;\n",
      "ANAh ste heuls, b be\n",
      "Thers thinp nmard thavovisoca yondr nove sf msrud li wamt snri:\n",
      "A\n",
      "The y ilrg.\n",
      "\n",
      "-\n",
      "RUAITI\n",
      "AO:\n",
      "Punirfel b, woy atheentor,,, nben Bryy g y bsds gL,\n",
      "Dorwig sout houlthd srhyomil, rro'ls he Sod-\n",
      "Bh'omir mssme,\n",
      "NI so, I'mir rerche chid lad, men msorep ons-.ON.\n",
      "\n",
      "\n",
      "LHche o cisled ta wt y aible s;\n",
      "Th gime I s;\n",
      "Ae?\n",
      "uguall:\n",
      "\n",
      "WEAhitiyr mhpy myicak bonce mtlomis eeocray sottachothi\n"
     ]
    }
   ],
   "source": [
    "_in = torch.zeros((1, 1), dtype=torch.long)\n",
    "out = m.generate(_in)\n",
    "print(\"\".join(decode(out[0].tolist())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd6cae19-849a-4c00-bc0a-fd0bf7486564",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
