{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "27923a2e-5509-46f4-9ce5-c20e4e565bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "246b6c96-b247-4c7c-9c0d-49bdf3bb4a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "089b6ead-17e9-4d4c-8c9b-6c5114a52969",
   "metadata": {},
   "outputs": [],
   "source": [
    "import v4_practice2 as vp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "12c8fa02-0502-4dcd-9abe-ba10e769ce6e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss 4.52384090423584 val loss 4.531491279602051\n",
      "train loss 2.586337089538574 val loss 2.6020822525024414\n",
      "train loss 2.443577527999878 val loss 2.4324381351470947\n",
      "train loss 2.354513168334961 val loss 2.3562426567077637\n",
      "train loss 2.2978646755218506 val loss 2.310206651687622\n",
      "train loss 2.2506797313690186 val loss 2.278728485107422\n",
      "train loss 2.251767635345459 val loss 2.26328182220459\n",
      "train loss 2.2026805877685547 val loss 2.226400136947632\n",
      "train loss 2.1891891956329346 val loss 2.228090763092041\n",
      "train loss 2.1551992893218994 val loss 2.2094316482543945\n",
      "train loss 2.1646649837493896 val loss 2.1777520179748535\n",
      "train loss 2.1382367610931396 val loss 2.1794638633728027\n",
      "train loss 2.1110422611236572 val loss 2.170823574066162\n",
      "train loss 2.1057891845703125 val loss 2.1614694595336914\n",
      "train loss 2.0848898887634277 val loss 2.140411853790283\n",
      "train loss 2.0794293880462646 val loss 2.1470770835876465\n",
      "train loss 2.0864768028259277 val loss 2.1454641819000244\n",
      "train loss 2.051305055618286 val loss 2.1086509227752686\n",
      "train loss 2.043764591217041 val loss 2.1187851428985596\n",
      "train loss 2.051313877105713 val loss 2.135929822921753\n",
      "\n",
      "Edivys:\n",
      "hat nown:\n",
      "Youlds.\n",
      "\n",
      "GLOUCETESHEN Rotherning\n",
      "the his mrommany.\n",
      "\n",
      "DUMR Comes?\n",
      "\n",
      "BRARIVES:\n",
      "Was crom'd's the sould truse to noter to blown:\n",
      "\n",
      "Jutomus,\n",
      "To lis, gonse himadins and at this of thath hord I pengincienors,\n",
      "KINT:\n",
      "Thif?\n",
      "Wither\n",
      "Mink awe rown hy,\n",
      "Brood, to of by well mageds good,\n",
      "To Rithinould agl:\n",
      "Ind him.\n",
      "\n",
      "RUCEORUY:\n",
      "But Hirnglidelicemore worke miersell shoulds;\n",
      "Thither:\n",
      "Tho more witen it hariess:\n",
      "Deet buct a thins,\n",
      "Anwittenfer cloon achope'lds like to kase to tone to et. Clich there a gerince:\n",
      "What enfer?\n",
      "\n",
      "CLONIO:\n",
      "Waptrems not your tone of on pone! into sell;\n",
      "As Pronay;\n",
      "Ays cripben a him hen say now shall herbyof.\n",
      "\n",
      "KING There sorton yours of knownod:\n",
      "weed\n",
      "We nesentrook come theive my llord say fortherer!\n",
      "\n",
      "DUKE ICAND:\n",
      "I me tencusitul wher shalt hath I shaly,\n",
      "To cithruchtis sas;\n",
      "To agon'd\n",
      "trown: in sor sihas sincien your of your sill:\n",
      "ppoin, not; here geft\n",
      "thin think gentrotourpamency ange thirgh, acy prow:\n",
      "Yor site pont.\n",
      "\n",
      "Lours gone him peellevity,\n",
      "Thoul yousen, and say;\n",
      "To you\n"
     ]
    }
   ],
   "source": [
    "with Path(\"../input.txt\").open(\"r\", encoding=\"utf-8\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "batch_size = 32\n",
    "block_size = 8\n",
    "n_embd = 32\n",
    "max_iters = 4000\n",
    "# max_iters = 1000\n",
    "eval_iters = 200\n",
    "\n",
    "data = vp.Data(text)\n",
    "data.block_size = block_size\n",
    "data.batch_size = batch_size\n",
    "\n",
    "m = vp.LanguageModel(\n",
    "    data.vocab_size,\n",
    "    n_embd=n_embd,\n",
    "    block_size=data.block_size\n",
    ")\n",
    "\n",
    "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)\n",
    "# m.train()\n",
    "for _iter in range(max_iters):\n",
    "    if _iter % eval_iters == 0:\n",
    "        out = data.estimate_loss(m, eval_iters)\n",
    "        print(f\"train loss {out['train']} val loss {out['val']}\")\n",
    "    xb, yb = data.get_batch('train')\n",
    "    logits, losses = m(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    losses.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "out = m.generate(torch.zeros((1, 1), dtype=torch.long))\n",
    "print(\"\".join(data.decode(out[0].tolist())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3c2e4c14-090b-43cf-a969-5b80030ecc45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import v4_practice2_layer_norm_neg1 as vpln"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4f3ab84e-d52d-4834-b773-14c2cbdde31d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss 4.300650596618652 val loss 4.3035101890563965\n",
      "train loss 2.5895323753356934 val loss 2.6094675064086914\n",
      "train loss 2.462552547454834 val loss 2.452244758605957\n",
      "train loss 2.3832905292510986 val loss 2.378441095352173\n",
      "train loss 2.323737144470215 val loss 2.3307201862335205\n",
      "train loss 2.278059482574463 val loss 2.300743341445923\n",
      "train loss 2.263681411743164 val loss 2.274761438369751\n",
      "train loss 2.217576503753662 val loss 2.243596076965332\n",
      "train loss 2.207119941711426 val loss 2.2373812198638916\n",
      "train loss 2.1738529205322266 val loss 2.2185535430908203\n",
      "train loss 2.184081792831421 val loss 2.1957876682281494\n",
      "train loss 2.159019708633423 val loss 2.1961774826049805\n",
      "train loss 2.128624200820923 val loss 2.181544780731201\n",
      "train loss 2.1253914833068848 val loss 2.174778699874878\n",
      "train loss 2.1092236042022705 val loss 2.148689031600952\n",
      "train loss 2.0991392135620117 val loss 2.1518959999084473\n",
      "train loss 2.107974052429199 val loss 2.1533043384552\n",
      "train loss 2.072219133377075 val loss 2.117452621459961\n",
      "train loss 2.057997941970825 val loss 2.126426935195923\n",
      "train loss 2.068138599395752 val loss 2.129685878753662\n",
      "\n",
      "Entire him thre compuls, to-forfall we youtly;\n",
      "I int of in mrhink bing nat and san\n",
      "Bithore ervay,\n",
      "to cantled the demmruse then to at that ton\n",
      "fet his, of list gonsen,\n",
      "Yaward and at this a my aghis clounden insill, this onselt. You his shink aws revanhy,\n",
      "Bromfalle of brows\n",
      "To a mahe madest solan: of havgligen'ds dees marce Yeferoper, soid licem, eld Enteriels in should this there\n",
      "fequere themen in harings:\n",
      "De ye folly thins aggentten thou son achope, me likn to kachate tonger; et.\n",
      "Clice there a gerine your maelf\n",
      "Angely trectespan, sundeck, rom nathem.\n",
      "\n",
      "No fore\n",
      "to stly pust\n",
      "it a maly kind benfalt in morratun'll the me sonoily, I post in his me yoursen:\n",
      "This od hat de on rumm!\n",
      "\n",
      "AECOPIOCEST:\n",
      "Hemy lexge\n",
      "say forteer, thy ponest!\n",
      "Do somenegsh Asell your shalt hath I shich crance or chall sack\n",
      "Thougon'd\n",
      "truppoming broude ser hat inther of your silil: poincon Plove, the tliff hing a gaony to thaw hay angry torgertedy prowmy\n",
      "drosst thou dell tan goned ontaly theith for thy wish with may too thou\n"
     ]
    }
   ],
   "source": [
    "with Path(\"../input.txt\").open(\"r\", encoding=\"utf-8\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "batch_size = 32\n",
    "block_size = 8\n",
    "n_embd = 32\n",
    "max_iters = 4000\n",
    "# max_iters = 1000\n",
    "eval_iters = 200\n",
    "\n",
    "data = vpln.Data(text)\n",
    "data.block_size = block_size\n",
    "data.batch_size = batch_size\n",
    "\n",
    "m = vpln.LanguageModel(\n",
    "    data.vocab_size,\n",
    "    n_embd=n_embd,\n",
    "    block_size=data.block_size\n",
    ")\n",
    "\n",
    "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)\n",
    "# m.train()\n",
    "for _iter in range(max_iters):\n",
    "    if _iter % eval_iters == 0:\n",
    "        out = data.estimate_loss(m, eval_iters)\n",
    "        print(f\"train loss {out['train']} val loss {out['val']}\")\n",
    "    xb, yb = data.get_batch('train')\n",
    "    logits, losses = m(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    losses.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "out = m.generate(torch.zeros((1, 1), dtype=torch.long))\n",
    "print(\"\".join(data.decode(out[0].tolist())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e51e6c9f-b399-4ed9-9a89-20a0a67721f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import v4_practice2_layer_norm_pytorch as vplnp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d5dc0b1-8d61-4abb-b1f2-273659aeb094",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss 4.305566310882568 val loss 4.308472156524658\n",
      "train loss 2.585562229156494 val loss 2.6051299571990967\n",
      "train loss 2.460526943206787 val loss 2.449514150619507\n",
      "train loss 2.378654956817627 val loss 2.3741676807403564\n",
      "train loss 2.319777011871338 val loss 2.3289711475372314\n",
      "train loss 2.274468183517456 val loss 2.2992353439331055\n",
      "train loss 2.260679244995117 val loss 2.2726964950561523\n",
      "train loss 2.2142333984375 val loss 2.241987705230713\n",
      "train loss 2.2027525901794434 val loss 2.235752582550049\n",
      "train loss 2.1666128635406494 val loss 2.2158143520355225\n",
      "train loss 2.178704261779785 val loss 2.1922526359558105\n",
      "train loss 2.1548304557800293 val loss 2.194735527038574\n",
      "train loss 2.125044584274292 val loss 2.182349920272827\n"
     ]
    }
   ],
   "source": [
    "with Path(\"../input.txt\").open(\"r\", encoding=\"utf-8\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "batch_size = 32\n",
    "block_size = 8\n",
    "n_embd = 32\n",
    "max_iters = 4000\n",
    "# max_iters = 1000\n",
    "eval_iters = 200\n",
    "\n",
    "data = vplnp.Data(text)\n",
    "data.block_size = block_size\n",
    "data.batch_size = batch_size\n",
    "\n",
    "m = vplnp.LanguageModel(\n",
    "    data.vocab_size,\n",
    "    n_embd=n_embd,\n",
    "    block_size=data.block_size\n",
    ")\n",
    "\n",
    "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)\n",
    "# m.train()\n",
    "for _iter in range(max_iters):\n",
    "    if _iter % eval_iters == 0:\n",
    "        out = data.estimate_loss(m, eval_iters)\n",
    "        print(f\"train loss {out['train']} val loss {out['val']}\")\n",
    "    xb, yb = data.get_batch('train')\n",
    "    logits, losses = m(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    losses.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "out = m.generate(torch.zeros((1, 1), dtype=torch.long))\n",
    "print(\"\".join(data.decode(out[0].tolist())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51aa8db5-d317-4558-b300-03e9154b1a92",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
