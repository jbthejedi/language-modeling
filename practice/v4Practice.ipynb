{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "27923a2e-5509-46f4-9ce5-c20e4e565bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "089b6ead-17e9-4d4c-8c9b-6c5114a52969",
   "metadata": {},
   "outputs": [],
   "source": [
    "import v4_practice2 as vp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eac92db8-d8fd-4b3e-ad8e-dd57759f7695",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vp.main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "246b6c96-b247-4c7c-9c0d-49bdf3bb4a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "12c8fa02-0502-4dcd-9abe-ba10e769ce6e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss 4.720187664031982 val loss 4.731351852416992\n",
      "train loss 2.6065731048583984 val loss 2.6093392372131348\n",
      "train loss 2.4346563816070557 val loss 2.4527337551116943\n",
      "train loss 2.385392665863037 val loss 2.40326189994812\n",
      "train loss 2.3213486671447754 val loss 2.321307420730591\n",
      "train loss 2.274491310119629 val loss 2.275160789489746\n",
      "train loss 2.2279675006866455 val loss 2.2550132274627686\n",
      "train loss 2.2182915210723877 val loss 2.229637861251831\n",
      "train loss 2.177237033843994 val loss 2.220369577407837\n",
      "train loss 2.1504933834075928 val loss 2.193065881729126\n",
      "train loss 2.1471805572509766 val loss 2.166292428970337\n",
      "train loss 2.1237637996673584 val loss 2.180574893951416\n",
      "train loss 2.099222421646118 val loss 2.163343906402588\n",
      "train loss 2.0931272506713867 val loss 2.146219253540039\n",
      "train loss 2.063145399093628 val loss 2.138683557510376\n",
      "train loss 2.070859909057617 val loss 2.1316165924072266\n",
      "train loss 2.0686113834381104 val loss 2.1487951278686523\n",
      "train loss 2.0295801162719727 val loss 2.1173276901245117\n",
      "train loss 2.038774013519287 val loss 2.122838020324707\n",
      "train loss 2.0196003913879395 val loss 2.114309549331665\n",
      "\n",
      "If this whillood pill!\n",
      "UThou deal, bines thes, but lordiounced to kis plill faried gaare;C hind\n",
      "Besed mir, what goo, any love'e earcy. Gor well\n",
      "O plowhing nons,\n",
      "Mend bet me hend?\n",
      "Whork eIV:\n",
      "ESA withmer yare\n",
      "Than her again so of Kill our mave eff at is promfurdes.\n",
      "\n",
      "RICENTER:\n",
      "Withice them bint\n",
      "Awith bre'st will and bedrouth: he eis his of wo, of thou must, I whuch moth, Yand for thee ardesing of have Henightake to hem flain, your\n",
      "Whould bose sold of but amash pack the do a greothere, Cin twith lown,\n",
      "Gon than he haris,\n",
      "That comes of enthere shonver his thee fath, I shall of that puld:\n",
      "Haget, with milsk sthome!\n",
      "Nor out thus\n",
      "When, I sees\n",
      "This donge\n",
      "Hour ham sin oof brous wilk thy?'\n",
      "\n",
      "CLANETBENCA:\n",
      "Grous, I hadie, I have a grofge offorse.\n",
      "Lecomeves eved your mings:\n",
      "That surk-flign youn.\n",
      "\n",
      "AOVLLIZAMENEENTIUS:\n",
      "UnIf whill\n",
      "Goknis, ady with sews weld Lardy:\n",
      "These, cu dain hupm evice my\n",
      "back';\n",
      "And think comen ove of scold upgain Rill;\n",
      "And ghturty the chave, thee one.\n",
      "\n",
      "GARD:\n",
      "And him of\n",
      "Bis hel:\n",
      "The co\n"
     ]
    }
   ],
   "source": [
    "with Path(\"../input.txt\").open(\"r\", encoding=\"utf-8\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "batch_size = 32\n",
    "block_size = 8\n",
    "n_embd = 32\n",
    "max_iters = 4000\n",
    "# max_iters = 1000\n",
    "eval_iters = 200\n",
    "\n",
    "data = vp.Data(text)\n",
    "data.block_size = block_size\n",
    "data.batch_size = batch_size\n",
    "\n",
    "m = vp.LanguageModel(\n",
    "    data.vocab_size,\n",
    "    n_embd=n_embd,\n",
    "    block_size=data.block_size\n",
    ")\n",
    "\n",
    "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)\n",
    "# m.train()\n",
    "for _iter in range(max_iters):\n",
    "    if _iter % eval_iters == 0:\n",
    "        out = data.estimate_loss(m, eval_iters)\n",
    "        print(f\"train loss {out['train']} val loss {out['val']}\")\n",
    "    xb, yb = data.get_batch('train')\n",
    "    logits, losses = m(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    losses.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "out = m.generate(torch.zeros((1, 1), dtype=torch.long))\n",
    "print(\"\".join(data.decode(out[0].tolist())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d2f82ef-656e-4960-a214-58fedd27f83c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e51e6c9f-b399-4ed9-9a89-20a0a67721f2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
