{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "27923a2e-5509-46f4-9ce5-c20e4e565bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "246b6c96-b247-4c7c-9c0d-49bdf3bb4a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "089b6ead-17e9-4d4c-8c9b-6c5114a52969",
   "metadata": {},
   "outputs": [],
   "source": [
    "import v4_practice2 as vp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "12c8fa02-0502-4dcd-9abe-ba10e769ce6e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss 4.52384090423584 val loss 4.531491279602051\n",
      "train loss 2.586337089538574 val loss 2.6020822525024414\n",
      "train loss 2.443577527999878 val loss 2.4324381351470947\n",
      "train loss 2.354513168334961 val loss 2.3562426567077637\n",
      "train loss 2.2978646755218506 val loss 2.310206651687622\n",
      "train loss 2.2506797313690186 val loss 2.278728485107422\n",
      "train loss 2.251767635345459 val loss 2.26328182220459\n",
      "train loss 2.2026805877685547 val loss 2.226400136947632\n",
      "train loss 2.1891891956329346 val loss 2.228090763092041\n",
      "train loss 2.1551992893218994 val loss 2.2094316482543945\n",
      "train loss 2.1646649837493896 val loss 2.1777520179748535\n",
      "train loss 2.1382367610931396 val loss 2.1794638633728027\n",
      "train loss 2.1110422611236572 val loss 2.170823574066162\n",
      "train loss 2.1057891845703125 val loss 2.1614694595336914\n",
      "train loss 2.0848898887634277 val loss 2.140411853790283\n",
      "train loss 2.0794293880462646 val loss 2.1470770835876465\n",
      "train loss 2.0864768028259277 val loss 2.1454641819000244\n",
      "train loss 2.051305055618286 val loss 2.1086509227752686\n",
      "train loss 2.043764591217041 val loss 2.1187851428985596\n",
      "train loss 2.051313877105713 val loss 2.135929822921753\n",
      "\n",
      "Edivys:\n",
      "hat nown:\n",
      "Youlds.\n",
      "\n",
      "GLOUCETESHEN Rotherning\n",
      "the his mrommany.\n",
      "\n",
      "DUMR Comes?\n",
      "\n",
      "BRARIVES:\n",
      "Was crom'd's the sould truse to noter to blown:\n",
      "\n",
      "Jutomus,\n",
      "To lis, gonse himadins and at this of thath hord I pengincienors,\n",
      "KINT:\n",
      "Thif?\n",
      "Wither\n",
      "Mink awe rown hy,\n",
      "Brood, to of by well mageds good,\n",
      "To Rithinould agl:\n",
      "Ind him.\n",
      "\n",
      "RUCEORUY:\n",
      "But Hirnglidelicemore worke miersell shoulds;\n",
      "Thither:\n",
      "Tho more witen it hariess:\n",
      "Deet buct a thins,\n",
      "Anwittenfer cloon achope'lds like to kase to tone to et. Clich there a gerince:\n",
      "What enfer?\n",
      "\n",
      "CLONIO:\n",
      "Waptrems not your tone of on pone! into sell;\n",
      "As Pronay;\n",
      "Ays cripben a him hen say now shall herbyof.\n",
      "\n",
      "KING There sorton yours of knownod:\n",
      "weed\n",
      "We nesentrook come theive my llord say fortherer!\n",
      "\n",
      "DUKE ICAND:\n",
      "I me tencusitul wher shalt hath I shaly,\n",
      "To cithruchtis sas;\n",
      "To agon'd\n",
      "trown: in sor sihas sincien your of your sill:\n",
      "ppoin, not; here geft\n",
      "thin think gentrotourpamency ange thirgh, acy prow:\n",
      "Yor site pont.\n",
      "\n",
      "Lours gone him peellevity,\n",
      "Thoul yousen, and say;\n",
      "To you\n"
     ]
    }
   ],
   "source": [
    "with Path(\"../input.txt\").open(\"r\", encoding=\"utf-8\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "batch_size = 32\n",
    "block_size = 8\n",
    "n_embd = 32\n",
    "max_iters = 4000\n",
    "# max_iters = 1000\n",
    "eval_iters = 200\n",
    "\n",
    "data = vp.Data(text)\n",
    "data.block_size = block_size\n",
    "data.batch_size = batch_size\n",
    "\n",
    "m = vp.LanguageModel(\n",
    "    data.vocab_size,\n",
    "    n_embd=n_embd,\n",
    "    block_size=data.block_size\n",
    ")\n",
    "\n",
    "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)\n",
    "# m.train()\n",
    "for _iter in range(max_iters):\n",
    "    if _iter % eval_iters == 0:\n",
    "        out = data.estimate_loss(m, eval_iters)\n",
    "        print(f\"train loss {out['train']} val loss {out['val']}\")\n",
    "    xb, yb = data.get_batch('train')\n",
    "    logits, losses = m(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    losses.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "out = m.generate(torch.zeros((1, 1), dtype=torch.long))\n",
    "print(\"\".join(data.decode(out[0].tolist())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3c2e4c14-090b-43cf-a969-5b80030ecc45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import v4_practice2_layer_norm_neg1 as vpln"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4f3ab84e-d52d-4834-b773-14c2cbdde31d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss 4.300650596618652 val loss 4.3035101890563965\n",
      "train loss 2.5895323753356934 val loss 2.6094675064086914\n",
      "train loss 2.462552547454834 val loss 2.452244758605957\n",
      "train loss 2.3832905292510986 val loss 2.378441095352173\n",
      "train loss 2.323737144470215 val loss 2.3307201862335205\n",
      "train loss 2.278059482574463 val loss 2.300743341445923\n",
      "train loss 2.263681411743164 val loss 2.274761438369751\n",
      "train loss 2.217576503753662 val loss 2.243596076965332\n",
      "train loss 2.207119941711426 val loss 2.2373812198638916\n",
      "train loss 2.1738529205322266 val loss 2.2185535430908203\n",
      "train loss 2.184081792831421 val loss 2.1957876682281494\n",
      "train loss 2.159019708633423 val loss 2.1961774826049805\n",
      "train loss 2.128624200820923 val loss 2.181544780731201\n",
      "train loss 2.1253914833068848 val loss 2.174778699874878\n",
      "train loss 2.1092236042022705 val loss 2.148689031600952\n",
      "train loss 2.0991392135620117 val loss 2.1518959999084473\n",
      "train loss 2.107974052429199 val loss 2.1533043384552\n",
      "train loss 2.072219133377075 val loss 2.117452621459961\n",
      "train loss 2.057997941970825 val loss 2.126426935195923\n",
      "train loss 2.068138599395752 val loss 2.129685878753662\n",
      "\n",
      "Entire him thre compuls, to-forfall we youtly;\n",
      "I int of in mrhink bing nat and san\n",
      "Bithore ervay,\n",
      "to cantled the demmruse then to at that ton\n",
      "fet his, of list gonsen,\n",
      "Yaward and at this a my aghis clounden insill, this onselt. You his shink aws revanhy,\n",
      "Bromfalle of brows\n",
      "To a mahe madest solan: of havgligen'ds dees marce Yeferoper, soid licem, eld Enteriels in should this there\n",
      "fequere themen in harings:\n",
      "De ye folly thins aggentten thou son achope, me likn to kachate tonger; et.\n",
      "Clice there a gerine your maelf\n",
      "Angely trectespan, sundeck, rom nathem.\n",
      "\n",
      "No fore\n",
      "to stly pust\n",
      "it a maly kind benfalt in morratun'll the me sonoily, I post in his me yoursen:\n",
      "This od hat de on rumm!\n",
      "\n",
      "AECOPIOCEST:\n",
      "Hemy lexge\n",
      "say forteer, thy ponest!\n",
      "Do somenegsh Asell your shalt hath I shich crance or chall sack\n",
      "Thougon'd\n",
      "truppoming broude ser hat inther of your silil: poincon Plove, the tliff hing a gaony to thaw hay angry torgertedy prowmy\n",
      "drosst thou dell tan goned ontaly theith for thy wish with may too thou\n"
     ]
    }
   ],
   "source": [
    "with Path(\"../input.txt\").open(\"r\", encoding=\"utf-8\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "batch_size = 32\n",
    "block_size = 8\n",
    "n_embd = 32\n",
    "max_iters = 4000\n",
    "# max_iters = 1000\n",
    "eval_iters = 200\n",
    "\n",
    "data = vpln.Data(text)\n",
    "data.block_size = block_size\n",
    "data.batch_size = batch_size\n",
    "\n",
    "m = vpln.LanguageModel(\n",
    "    data.vocab_size,\n",
    "    n_embd=n_embd,\n",
    "    block_size=data.block_size\n",
    ")\n",
    "\n",
    "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)\n",
    "# m.train()\n",
    "for _iter in range(max_iters):\n",
    "    if _iter % eval_iters == 0:\n",
    "        out = data.estimate_loss(m, eval_iters)\n",
    "        print(f\"train loss {out['train']} val loss {out['val']}\")\n",
    "    xb, yb = data.get_batch('train')\n",
    "    logits, losses = m(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    losses.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "out = m.generate(torch.zeros((1, 1), dtype=torch.long))\n",
    "print(\"\".join(data.decode(out[0].tolist())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e51e6c9f-b399-4ed9-9a89-20a0a67721f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import v4_practice2_layer_norm_pytorch as vplnp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8d5dc0b1-8d61-4abb-b1f2-273659aeb094",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss 4.305566310882568 val loss 4.308472156524658\n",
      "train loss 2.585562229156494 val loss 2.6051299571990967\n",
      "train loss 2.460526943206787 val loss 2.449514150619507\n",
      "train loss 2.378654956817627 val loss 2.3741676807403564\n",
      "train loss 2.319777011871338 val loss 2.3289711475372314\n",
      "train loss 2.274468183517456 val loss 2.2992353439331055\n",
      "train loss 2.260679244995117 val loss 2.2726964950561523\n",
      "train loss 2.2142333984375 val loss 2.241987705230713\n",
      "train loss 2.2027525901794434 val loss 2.235752582550049\n",
      "train loss 2.1666128635406494 val loss 2.2158143520355225\n",
      "train loss 2.178704261779785 val loss 2.1922526359558105\n",
      "train loss 2.1548304557800293 val loss 2.194735527038574\n",
      "train loss 2.125044584274292 val loss 2.182349920272827\n",
      "train loss 2.1225156784057617 val loss 2.1751301288604736\n",
      "train loss 2.102186679840088 val loss 2.1464028358459473\n",
      "train loss 2.0904812812805176 val loss 2.1471848487854004\n",
      "train loss 2.098524570465088 val loss 2.1469109058380127\n",
      "train loss 2.0648531913757324 val loss 2.1114015579223633\n",
      "train loss 2.0530028343200684 val loss 2.1251306533813477\n",
      "train loss 2.0615291595458984 val loss 2.1265387535095215\n",
      "\n",
      "Entire hem thre compuant.\n",
      "\n",
      "GLANUTE:\n",
      "He youth.\n",
      "nill\n",
      "Whe his mrome,\n",
      "Omerent nony santl thougher appurs, witled the demmruse then to ack; lown:\n",
      "\n",
      "Juthe De, and so goysen,\n",
      "Yad my fortaty siccong,\n",
      "aghich lourden insil,\n",
      "Whis an so the dinher shink aws to tohy,\n",
      "Brovan, of fore well mage hing Rear so.\n",
      "\n",
      "proy oxagl:\n",
      "Ifars dees marce Yeferoper, soid lick,-beld, kne is sell should this there\n",
      "ferreppent,\n",
      "With prariess:\n",
      "De\n",
      "WINGTONIUS:\n",
      "GsAy good yife welll, achoperventl Bo to kachat tronger; et.\n",
      "Clich there a gerince the maelf\n",
      "Angely trectespan, sunst yincomponouer.\n",
      "\n",
      "No fore\n",
      "to sell;\n",
      "As Propaghtord.\n",
      "Offoon all in morratun'll the me soyof youly;\n",
      "The ab. thou quearnow, not od hat deep hesel!\n",
      "\n",
      "AECOPIO:\n",
      "Litht my lext marting to serxinh my struposs fet grod-denl where to thy congens.\n",
      "If to cimer chall sack!\n",
      "\n",
      "WAMMENLY:\n",
      "But infing blimase, hat inther of your diding, githon Plave, the thith lung angen:\n",
      "Good, faincy angrettlung, for prow:\n",
      "Yor site you dell tay goned ontaly theither'd thy wish with may:\n",
      "Onoo'l \n"
     ]
    }
   ],
   "source": [
    "with Path(\"../input.txt\").open(\"r\", encoding=\"utf-8\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "batch_size = 32\n",
    "block_size = 8\n",
    "n_embd = 32\n",
    "max_iters = 4000\n",
    "# max_iters = 1000\n",
    "eval_iters = 200\n",
    "\n",
    "data = vplnp.Data(text)\n",
    "data.block_size = block_size\n",
    "data.batch_size = batch_size\n",
    "\n",
    "m = vplnp.LanguageModel(\n",
    "    data.vocab_size,\n",
    "    n_embd=n_embd,\n",
    "    block_size=data.block_size\n",
    ")\n",
    "\n",
    "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)\n",
    "# m.train()\n",
    "for _iter in range(max_iters):\n",
    "    if _iter % eval_iters == 0:\n",
    "        out = data.estimate_loss(m, eval_iters)\n",
    "        print(f\"train loss {out['train']} val loss {out['val']}\")\n",
    "    xb, yb = data.get_batch('train')\n",
    "    logits, losses = m(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    losses.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "out = m.generate(torch.zeros((1, 1), dtype=torch.long))\n",
    "print(\"\".join(data.decode(out[0].tolist())))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5196f5cc-0008-420e-be96-5e22aa001d55",
   "metadata": {},
   "source": [
    "### Incorrect implementation with x.mean(1, ...) instead of x.mean(-1, ...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ba3dfa78-2d32-4e27-b4f5-848d7a27a02d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import v4_practice2_layer_norm_1_incorrect as vplni"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "87913e77-589f-4e7c-a507-7544af939398",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss 4.298152923583984 val loss 4.29722261428833\n",
      "train loss 2.058264970779419 val loss 2.116945743560791\n",
      "train loss 1.103388786315918 val loss 1.1344937086105347\n",
      "train loss 0.70946204662323 val loss 0.7278688549995422\n",
      "train loss 0.5507558584213257 val loss 0.5712594389915466\n",
      "train loss 0.47247105836868286 val loss 0.4944007396697998\n",
      "train loss 0.4445202648639679 val loss 0.46617603302001953\n",
      "train loss 0.405193567276001 val loss 0.4278334677219391\n",
      "train loss 0.4031125605106354 val loss 0.4270722270011902\n",
      "train loss 0.3865523040294647 val loss 0.40436944365501404\n",
      "train loss 0.3811471164226532 val loss 0.3914058208465576\n",
      "train loss 0.3679414689540863 val loss 0.39009860157966614\n",
      "train loss 0.35603636503219604 val loss 0.3738629221916199\n",
      "train loss 0.37202614545822144 val loss 0.39234665036201477\n",
      "train loss 0.3532384932041168 val loss 0.3712049722671509\n",
      "train loss 0.3498852849006653 val loss 0.3615388572216034\n",
      "train loss 0.35515254735946655 val loss 0.36008235812187195\n",
      "train loss 0.34508830308914185 val loss 0.36324501037597656\n",
      "train loss 0.3462570309638977 val loss 0.3636256754398346\n",
      "train loss 0.3290572464466095 val loss 0.3430248200893402\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/justinbarry/projects/language-modeling/practice/v4_practice2_layer_norm_1_incorrect.py:57: UserWarning: var(): degrees of freedom is <= 0. Correction should be strictly less than the reduction factor (input numel divided by output numel). (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/ReduceOps.cpp:1823.)\n",
      "  xvar = x.var(1, keepdim=True)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "probability tensor contains either `inf`, `nan` or element < 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 33\u001b[0m\n\u001b[1;32m     30\u001b[0m     losses\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     31\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m---> 33\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzeros\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlong\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(data\u001b[38;5;241m.\u001b[39mdecode(out[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mtolist())))\n",
      "File \u001b[0;32m~/projects/language-modeling/practice/v4_practice2_layer_norm_1_incorrect.py:170\u001b[0m, in \u001b[0;36mLanguageModel.generate\u001b[0;34m(self, idx, max_new_tokens)\u001b[0m\n\u001b[1;32m    168\u001b[0m     logits \u001b[38;5;241m=\u001b[39m logits[:,\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,:]\n\u001b[1;32m    169\u001b[0m     probs \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39msoftmax(logits, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m--> 170\u001b[0m     idx_next \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmultinomial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    171\u001b[0m     idx \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([idx, idx_next], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    172\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m idx\n",
      "\u001b[0;31mRuntimeError\u001b[0m: probability tensor contains either `inf`, `nan` or element < 0"
     ]
    }
   ],
   "source": [
    "with Path(\"../input.txt\").open(\"r\", encoding=\"utf-8\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "batch_size = 32\n",
    "block_size = 8\n",
    "n_embd = 32\n",
    "max_iters = 4000\n",
    "# max_iters = 1000\n",
    "eval_iters = 200\n",
    "\n",
    "data = vplni.Data(text)\n",
    "data.block_size = block_size\n",
    "data.batch_size = batch_size\n",
    "\n",
    "m = vplni.LanguageModel(\n",
    "    data.vocab_size,\n",
    "    n_embd=n_embd,\n",
    "    block_size=data.block_size\n",
    ")\n",
    "\n",
    "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)\n",
    "# m.train()\n",
    "for _iter in range(max_iters):\n",
    "    if _iter % eval_iters == 0:\n",
    "        out = data.estimate_loss(m, eval_iters)\n",
    "        print(f\"train loss {out['train']} val loss {out['val']}\")\n",
    "    xb, yb = data.get_batch('train')\n",
    "    logits, losses = m(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    losses.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "out = m.generate(torch.zeros((1, 1), dtype=torch.long))\n",
    "print(\"\".join(data.decode(out[0].tolist())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51aa8db5-d317-4558-b300-03e9154b1a92",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
