{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4cd4adec-bf6f-41bd-bf07-75d1d3a8af21",
   "metadata": {},
   "source": [
    "# Self attention example implementations\n",
    "\n",
    "Query is \"what am I looking for?\"  \n",
    "Key is \"what do I contain?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8fc1531c-d001-45fb-8a83-1f7cc5371724",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x119dedef0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "torch.manual_seed(1337)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6d55aec9-5c4e-498b-8836-3c73d58db5ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 1., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 1., 1., 0., 0., 0., 0., 0.],\n",
       "        [1., 1., 1., 1., 0., 0., 0., 0.],\n",
       "        [1., 1., 1., 1., 1., 0., 0., 0.],\n",
       "        [1., 1., 1., 1., 1., 1., 0., 0.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 0.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B, T, C = 4, 8, 2\n",
    "x = torch.randn(B, T, C)\n",
    "wei = torch.tril(torch.ones(T, T))\n",
    "wei"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "78149793-4097-49af-a0f1-1ffa57562b45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],\n",
       "        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],\n",
       "        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wei = wei / wei.sum(1, keepdim=True)\n",
    "wei"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ce95412e-e8db-4aa1-94b0-ac75d72584b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-2.0555,  1.8275],\n",
       "         [-0.3760,  0.6887],\n",
       "         [ 0.1984,  1.0228],\n",
       "         [ 0.1177,  0.3465],\n",
       "         [ 0.0888,  0.2920],\n",
       "         [ 0.2493,  0.3563],\n",
       "         [ 0.2575,  0.1987],\n",
       "         [ 0.3182,  0.2848]],\n",
       "\n",
       "        [[ 2.2874,  0.9611],\n",
       "         [ 0.3789,  0.3350],\n",
       "         [ 0.2146,  0.1187],\n",
       "         [ 0.0036,  0.3737],\n",
       "         [-0.1954,  0.3329],\n",
       "         [ 0.0413,  0.2384],\n",
       "         [-0.1156,  0.1108],\n",
       "         [ 0.0977,  0.0096]],\n",
       "\n",
       "        [[-0.8961,  0.0662],\n",
       "         [-0.4762,  1.2037],\n",
       "         [-1.2253,  0.9724],\n",
       "         [-1.1226,  0.6678],\n",
       "         [-0.8971,  0.9437],\n",
       "         [-0.7739,  0.7500],\n",
       "         [-0.8565,  0.6346],\n",
       "         [-0.9811,  0.3822]],\n",
       "\n",
       "        [[-0.3454, -1.1625],\n",
       "         [-0.1005, -0.4981],\n",
       "         [ 0.1833, -0.0277],\n",
       "         [-0.2945,  0.3056],\n",
       "         [-0.0437,  0.4565],\n",
       "         [ 0.0685,  0.1660],\n",
       "         [-0.0395,  0.4477],\n",
       "         [ 0.0294,  0.5441]]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xbow = wei @ x # (T, T) x (B, T, C) = (B, T, C), for dimension B is preserved\n",
    "xbow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b535942-40a2-4b53-a519-b1d31046299f",
   "metadata": {},
   "source": [
    "### Third way (one we will use)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8c228a1b-1c57-4dd9-b35d-80fbd64fbc9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-2.0555,  1.8275],\n",
       "         [-0.3760,  0.6887],\n",
       "         [ 0.1984,  1.0228],\n",
       "         [ 0.1177,  0.3465],\n",
       "         [ 0.0888,  0.2920],\n",
       "         [ 0.2493,  0.3563],\n",
       "         [ 0.2575,  0.1987],\n",
       "         [ 0.3182,  0.2848]],\n",
       "\n",
       "        [[ 2.2874,  0.9611],\n",
       "         [ 0.3789,  0.3350],\n",
       "         [ 0.2146,  0.1187],\n",
       "         [ 0.0036,  0.3737],\n",
       "         [-0.1954,  0.3329],\n",
       "         [ 0.0413,  0.2384],\n",
       "         [-0.1156,  0.1108],\n",
       "         [ 0.0977,  0.0096]],\n",
       "\n",
       "        [[-0.8961,  0.0662],\n",
       "         [-0.4762,  1.2037],\n",
       "         [-1.2253,  0.9724],\n",
       "         [-1.1226,  0.6678],\n",
       "         [-0.8971,  0.9437],\n",
       "         [-0.7739,  0.7500],\n",
       "         [-0.8565,  0.6346],\n",
       "         [-0.9811,  0.3822]],\n",
       "\n",
       "        [[-0.3454, -1.1625],\n",
       "         [-0.1005, -0.4981],\n",
       "         [ 0.1833, -0.0277],\n",
       "         [-0.2945,  0.3056],\n",
       "         [-0.0437,  0.4565],\n",
       "         [ 0.0685,  0.1660],\n",
       "         [-0.0395,  0.4477],\n",
       "         [ 0.0294,  0.5441]]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tril = torch.tril(torch.ones(T, T))\n",
    "wei = torch.zeros((T, T))\n",
    "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
    "wei = F.softmax(wei, dim=-1)\n",
    "xbow3 = wei @ x \n",
    "xbow3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eedc720-8be5-4b65-855e-c3957fc5b28f",
   "metadata": {},
   "source": [
    "### Single self-attention head\n",
    "\n",
    "Note: self-attention means q, k, v all come from same source x.  \n",
    "Cross attention means there's the keys and values come from a separate source\n",
    "\n",
    "Because of softmax, if wei takes on very position or very negative numbers inside of it, softmax will  \n",
    "converge to one-hot vectors. IOW: if the numbers inside a vector are more diffuse (evenly spread out),\n",
    "softmax won't tend to converge toward a single value. If the numbers are very negative and positive during\n",
    "initialization, then softmax will conversge to a single value, basically saying \"only pay attention to this value\".  \n",
    "So the sqrt(d) scaling is used to control the variance\n",
    "of softmax during initialization\n",
    "\n",
    "This is why we scale by sqrt(d).\n",
    "iow: the sqrt(d) scaling is used to control the variance at initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3b22540d-4abb-4f97-885a-f6aa8fe4282f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 6.6016e-02,  8.6541e-02, -2.1800e-03, -9.7871e-02,  4.9378e-02,\n",
       "          -8.4692e-02, -1.6165e-01, -4.9517e-02,  1.2838e-01,  1.3316e-01,\n",
       "           9.1477e-03,  5.9705e-02,  1.5792e-01, -3.8152e-02,  4.1841e-02,\n",
       "          -8.9396e-02],\n",
       "         [-2.5548e-01,  1.1884e-01, -2.2966e-01, -1.9912e-01,  3.3471e-01,\n",
       "           1.5141e-01, -2.4099e-01,  7.8147e-02,  2.9808e-02,  2.5287e-01,\n",
       "           1.9010e-01, -9.2274e-02,  2.7042e-01, -6.0876e-02, -1.4815e-01,\n",
       "          -2.5797e-01],\n",
       "         [-2.7583e-02,  1.5441e-01, -9.9084e-02, -2.0180e-01,  2.0019e-01,\n",
       "          -3.8674e-02, -2.9640e-01, -2.6971e-02,  1.6753e-01,  2.6698e-01,\n",
       "           9.0885e-02,  3.3340e-02,  3.0425e-01, -7.1635e-02, -1.1698e-02,\n",
       "          -2.1629e-01],\n",
       "         [ 1.5503e-01,  2.0607e-01, -6.6096e-03, -2.3345e-01,  1.1925e-01,\n",
       "          -1.9999e-01, -3.8504e-01, -1.1699e-01,  3.0476e-01,  3.1750e-01,\n",
       "           2.2893e-02,  1.4108e-01,  3.7636e-01, -9.0899e-02,  9.8343e-02,\n",
       "          -2.1371e-01],\n",
       "         [ 1.8211e-01,  1.1709e-01,  5.7809e-02, -1.1507e-01, -4.7351e-03,\n",
       "          -1.8635e-01, -2.1363e-01, -1.0617e-01,  2.1296e-01,  1.6140e-01,\n",
       "          -3.5213e-02,  1.2749e-01,  1.9931e-01, -4.9347e-02,  1.1173e-01,\n",
       "          -8.4714e-02],\n",
       "         [-1.5710e-01, -2.2076e-02, -9.1293e-02, -1.2596e-03,  9.5560e-02,\n",
       "           1.3009e-01,  3.3533e-02,  7.1851e-02, -9.2104e-02, -5.5945e-03,\n",
       "           6.9610e-02, -8.5844e-02, -1.8580e-02,  6.2942e-03, -9.3991e-02,\n",
       "          -3.1984e-02],\n",
       "         [ 6.4432e-02,  6.5739e-02,  7.6976e-03, -7.1675e-02,  2.6495e-02,\n",
       "          -7.5382e-02, -1.2201e-01, -4.3645e-02,  1.0356e-01,  9.8260e-02,\n",
       "          -3.7754e-04,  5.2545e-02,  1.1775e-01, -2.8632e-02,  4.0269e-02,\n",
       "          -6.2329e-02],\n",
       "         [-1.7010e-01, -1.3525e-01, -4.0418e-02,  1.4045e-01, -2.5565e-02,\n",
       "           1.8412e-01,  2.4897e-01,  1.0564e-01, -2.2895e-01, -1.9457e-01,\n",
       "           2.0030e-02, -1.2700e-01, -2.3645e-01,  5.7987e-02, -1.0515e-01,\n",
       "           1.1357e-01]],\n",
       "\n",
       "        [[ 6.6151e-01,  5.2770e-01,  1.5629e-01, -5.4836e-01,  1.0140e-01,\n",
       "          -7.1671e-01, -9.7148e-01, -4.1127e-01,  8.9238e-01,  7.5955e-01,\n",
       "          -7.7050e-02,  4.9444e-01,  9.2286e-01, -2.2629e-01,  4.0896e-01,\n",
       "          -4.4395e-01],\n",
       "         [ 6.2176e-01,  1.7783e-01,  3.1384e-01, -1.1022e-01, -2.7337e-01,\n",
       "          -5.5000e-01, -3.0548e-01, -3.0699e-01,  4.6951e-01,  1.7529e-01,\n",
       "          -2.3054e-01,  3.6741e-01,  2.4928e-01, -6.6482e-02,  3.7473e-01,\n",
       "           6.1848e-03],\n",
       "         [-6.0287e-01, -7.1644e-01, -1.8854e-02,  7.9970e-01, -3.6533e-01,\n",
       "           7.4471e-01,  1.3352e+00,  4.3372e-01, -1.0866e+00, -1.0909e+00,\n",
       "          -4.6827e-02, -5.2264e-01, -1.2986e+00,  3.1447e-01, -3.7986e-01,\n",
       "           7.1806e-01],\n",
       "         [ 6.7588e-01,  2.1005e-01,  3.3238e-01, -1.4114e-01, -2.7777e-01,\n",
       "          -6.0438e-01, -3.6404e-01, -3.3790e-01,  5.2980e-01,  2.1889e-01,\n",
       "          -2.4228e-01,  4.0452e-01,  3.0351e-01, -7.9963e-02,  4.0786e-01,\n",
       "          -1.5560e-02],\n",
       "         [ 3.8799e-01,  1.6245e-01,  1.6883e-01, -1.3434e-01, -1.1093e-01,\n",
       "          -3.6322e-01, -2.8894e-01, -2.0444e-01,  3.5273e-01,  1.9654e-01,\n",
       "          -1.1827e-01,  2.4502e-01,  2.5558e-01, -6.5145e-02,  2.3540e-01,\n",
       "          -6.4661e-02],\n",
       "         [-3.0944e-01, -2.3304e-01, -8.0352e-02,  2.3893e-01, -3.1435e-02,\n",
       "           3.2989e-01,  4.2807e-01,  1.8893e-01, -4.0141e-01, -3.3193e-01,\n",
       "           4.2903e-02, -2.2706e-01, -4.0487e-01,  9.9509e-02, -1.9088e-01,\n",
       "           1.8929e-01],\n",
       "         [ 1.9598e-01,  1.6638e-01,  4.1031e-02, -1.7525e-01,  4.1680e-02,\n",
       "          -2.1623e-01, -3.0699e-01, -1.2435e-01,  2.7603e-01,  2.4203e-01,\n",
       "          -1.7834e-02,  1.4955e-01,  2.9292e-01, -7.1656e-02,  1.2146e-01,\n",
       "          -1.4489e-01],\n",
       "         [ 2.4823e-01,  1.5587e-01,  8.0764e-02, -1.5209e-01, -1.0794e-02,\n",
       "          -2.5256e-01, -2.8404e-01, -1.4379e-01,  2.8594e-01,  2.1366e-01,\n",
       "          -4.9860e-02,  1.7264e-01,  2.6441e-01, -6.5544e-02,  1.5218e-01,\n",
       "          -1.1049e-01]],\n",
       "\n",
       "        [[-4.6647e-01, -1.6072e-01, -2.2113e-01,  1.1747e-01,  1.7346e-01,\n",
       "           4.2324e-01,  2.8133e-01,  2.3715e-01, -3.8393e-01, -1.7773e-01,\n",
       "           1.5939e-01, -2.8400e-01, -2.4007e-01,  6.2426e-02, -2.8197e-01,\n",
       "           3.1701e-02],\n",
       "         [ 2.1280e-01,  1.5031e-01,  6.0480e-02, -1.5164e-01,  1.0087e-02,\n",
       "          -2.2300e-01, -2.7538e-01, -1.2744e-01,  2.6450e-01,  2.1142e-01,\n",
       "          -3.4450e-02,  1.5311e-01,  2.5910e-01, -6.3860e-02,  1.3097e-01,\n",
       "          -1.1693e-01],\n",
       "         [ 1.9021e-01, -4.9058e-02,  1.5030e-01,  9.8044e-02, -2.0352e-01,\n",
       "          -1.2805e-01,  1.0414e-01, -6.8041e-02,  2.3558e-02, -1.2153e-01,\n",
       "          -1.2195e-01,  8.0757e-02, -1.2474e-01,  2.7208e-02,  1.1150e-01,\n",
       "           1.3960e-01],\n",
       "         [-1.7105e-01, -1.6998e-01, -2.2814e-02,  1.8450e-01, -6.5082e-02,\n",
       "           1.9835e-01,  3.1525e-01,  1.1473e-01, -2.6967e-01, -2.5318e-01,\n",
       "           3.2552e-03, -1.3810e-01, -3.0378e-01,  7.3925e-02, -1.0676e-01,\n",
       "           1.5943e-01],\n",
       "         [-1.4162e-01, -8.1297e-02, -5.0080e-02,  7.7056e-02,  1.4997e-02,\n",
       "           1.4113e-01,  1.4749e-01,  8.0126e-02, -1.5428e-01, -1.0898e-01,\n",
       "           3.2237e-02, -9.6164e-02, -1.3603e-01,  3.3889e-02, -8.6592e-02,\n",
       "           5.2884e-02],\n",
       "         [-1.1557e-02,  9.6548e-02, -5.8229e-02, -1.2512e-01,  1.2079e-01,\n",
       "          -2.8584e-02, -1.8503e-01, -1.9267e-02,  1.0716e-01,  1.6579e-01,\n",
       "           5.3910e-02,  2.3712e-02,  1.8936e-01, -4.4653e-02, -3.9345e-03,\n",
       "          -1.3302e-01],\n",
       "         [-3.5572e-01, -1.4969e-01, -1.5439e-01,  1.2413e-01,  1.0084e-01,\n",
       "           3.3330e-01,  2.6634e-01,  1.8763e-01, -3.2426e-01, -1.8146e-01,\n",
       "           1.0806e-01, -2.2487e-01, -2.3578e-01,  6.0071e-02, -2.1585e-01,\n",
       "           6.0281e-02],\n",
       "         [-2.8231e-01, -7.5208e-02, -1.4540e-01,  4.2995e-02,  1.3054e-01,\n",
       "           2.4757e-01,  1.2813e-01,  1.3800e-01, -2.0675e-01, -7.0217e-02,\n",
       "           1.0743e-01, -1.6513e-01, -1.0243e-01,  2.7642e-02, -1.6998e-01,\n",
       "          -1.0176e-02]],\n",
       "\n",
       "        [[ 5.2768e-01,  8.3895e-01, -9.4672e-02, -9.6978e-01,  5.6528e-01,\n",
       "          -7.3417e-01, -1.5733e+00, -4.3261e-01,  1.1970e+00,  1.3136e+00,\n",
       "           1.4628e-01,  5.2226e-01,  1.5483e+00, -3.7261e-01,  3.3891e-01,\n",
       "          -9.1050e-01],\n",
       "         [ 5.6255e-01,  7.1428e-01, -6.4164e-03, -8.0449e-01,  3.9392e-01,\n",
       "          -7.1269e-01, -1.3333e+00, -4.1616e-01,  1.0670e+00,  1.0954e+00,\n",
       "           6.6435e-02,  5.0169e-01,  1.3007e+00, -3.1446e-01,  3.5584e-01,\n",
       "          -7.3094e-01],\n",
       "         [ 2.1543e-01, -7.8805e-01,  5.5459e-01,  1.0439e+00, -1.0793e+00,\n",
       "           1.3965e-01,  1.5169e+00,  1.0613e-01, -8.2345e-01, -1.3777e+00,\n",
       "          -5.0215e-01, -1.3258e-01, -1.5644e+00,  3.6744e-01,  1.0405e-01,\n",
       "           1.1330e+00],\n",
       "         [ 6.5572e-01,  1.2416e+00, -2.2211e-01, -1.4586e+00,  9.3314e-01,\n",
       "          -9.8968e-01, -2.3352e+00, -5.8738e-01,  1.7185e+00,  1.9693e+00,\n",
       "           2.8072e-01,  7.0988e-01,  2.3108e+00, -5.5452e-01,  4.2719e-01,\n",
       "          -1.3964e+00],\n",
       "         [ 5.3477e-01,  1.0383e+00, -1.9464e-01, -1.2224e+00,  7.9082e-01,\n",
       "          -8.1713e-01, -1.9536e+00, -4.8547e-01,  1.4314e+00,  1.6496e+00,\n",
       "           2.4172e-01,  5.8681e-01,  1.9345e+00, -4.6406e-01,  3.4918e-01,\n",
       "          -1.1731e+00],\n",
       "         [ 4.6193e-01,  7.8798e-01, -1.1098e-01, -9.1717e-01,  5.5692e-01,\n",
       "          -6.6351e-01, -1.4795e+00, -3.9211e-01,  1.1100e+00,  1.2406e+00,\n",
       "           1.5468e-01,  4.7357e-01,  1.4594e+00, -3.5080e-01,  2.9831e-01,\n",
       "          -8.6835e-01],\n",
       "         [ 5.0548e-01,  6.0913e-01,  1.1387e-02, -6.8124e-01,  3.1607e-01,\n",
       "          -6.2768e-01, -1.1356e+00, -3.6576e-01,  9.2086e-01,  9.2897e-01,\n",
       "           4.3449e-02,  4.4079e-01,  1.1052e+00, -2.6754e-01,  3.1875e-01,\n",
       "          -6.1328e-01],\n",
       "         [ 5.8444e-01,  8.2216e-01, -4.8688e-02, -9.3779e-01,  5.0205e-01,\n",
       "          -7.7155e-01, -1.5381e+00, -4.5238e-01,  1.2015e+00,  1.2737e+00,\n",
       "           1.0882e-01,  5.4571e-01,  1.5069e+00, -3.6350e-01,  3.7212e-01,\n",
       "          -8.6598e-01]]], grad_fn=<UnsafeViewBackward0>)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "B, T, C = 4, 8, 2\n",
    "x = torch.randn(B, T, C)\n",
    "\n",
    "# Single head\n",
    "head_size = 16\n",
    "\n",
    "# initialized weights between key and query will be different\n",
    "# thus yeilding differeing values for each item in the batch\n",
    "key = nn.Linear(C, head_size, bias=False) # inits with weights here\n",
    "query = nn.Linear(C, head_size, bias=False)\n",
    "value = nn.Linear(C, head_size, bias=False) \n",
    "\n",
    "k = key(x)   # (B, T, 16) - what do I contain?\n",
    "q = query(x) # (B, T, 16) - what am I looking for?\n",
    "\n",
    "# For each item in the batch, we'll have a TxT affinity matrix\n",
    "wei = q @ k.transpose(-2, -1) * head_size**-0.5 # (B, T, 16) x (B, 16, T) --> (B, T, T)\n",
    "\n",
    "tril = torch.tril(torch.ones(T, T))\n",
    "\n",
    "# Mask so we don't use information from the future\n",
    "# NOTE: only diff bw encoder and decoder\n",
    "# is encoder has this line commented out\n",
    "wei = wei.masked_fill(tril == 0, float('-inf')) \n",
    "\n",
    "wei = F.softmax(wei, dim=-1)\n",
    "\n",
    "# Here is what I will communicate to you (between different heads. Gives head unique values).\n",
    "v = value(x) # (B, T, 16)\n",
    "out = wei @ v # (B, T, T) x (B, T, C) = (B, T, C), for dimension B is preserved\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "152c491c-65c4-4cea-8b1d-4205886be8cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.5599, 0.4401, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.3220, 0.2016, 0.4764, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1640, 0.0815, 0.2961, 0.4585, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2051, 0.3007, 0.1894, 0.1808, 0.1241, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0600, 0.1273, 0.0291, 0.0169, 0.0552, 0.7114, 0.0000, 0.0000],\n",
       "        [0.1408, 0.1025, 0.1744, 0.2038, 0.1690, 0.0669, 0.1426, 0.0000],\n",
       "        [0.0223, 0.1086, 0.0082, 0.0040, 0.0080, 0.7257, 0.0216, 0.1016]],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wei[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ca6b302-656f-45de-8de4-2f09386ccc45",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
